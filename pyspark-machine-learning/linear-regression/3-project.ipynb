{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local\").\\\n",
    "    appName('linear-regression-project').\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|\n",
      "|    Ecstasy|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Elation|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Fantasy|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23| 9.2|\n",
      "|Fascination|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Freedom|   Carnival|  6|110.23899999999999|      37.0|  9.51| 14.87|            29.79|11.5|\n",
      "|      Glory|   Carnival| 10|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Holiday|   Carnival| 28|            46.052|     14.52|  7.27|  7.26|            31.72| 6.6|\n",
      "|Imagination|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|Inspiration|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|     Legend|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49| 9.3|\n",
      "|   Liberty*|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Miracle|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67|10.3|\n",
      "|   Paradise|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|      Pride|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67| 9.3|\n",
      "|  Sensation|   Carnival| 20|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    path='D:/learn-ab/learning-PySpark/sample-data/cruise-ship-info.csv',\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ship_name: string (nullable = true)\n",
      " |-- Cruise_line: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tonnage: double (nullable = true)\n",
      " |-- passengers: double (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- cabins: double (nullable = true)\n",
      " |-- passenger_density: double (nullable = true)\n",
      " |-- crew: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|Ship_name|Cruise_line|               Age|           Tonnage|       passengers|           length|            cabins|passenger_density|             crew|\n",
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|      158|        158|               158|               158|              158|              158|               158|              158|              158|\n",
      "|   mean| Infinity|       NULL|15.689873417721518| 71.28467088607599|18.45740506329114|8.130632911392404| 8.830000000000005|39.90094936708861|7.794177215189873|\n",
      "| stddev|     NULL|       NULL| 7.615691058751413|37.229540025907866|9.677094775143416|1.793473548054825|4.4714172221480615| 8.63921711391542|3.503486564627034|\n",
      "|    min|Adventure|    Azamara|                 4|             2.329|             0.66|             2.79|              0.33|             17.7|             0.59|\n",
      "|    max|Zuiderdam|   Windstar|                48|             220.0|             54.0|            11.82|              27.0|            71.43|             21.0|\n",
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|      Cruise_line|count|\n",
      "+-----------------+-----+\n",
      "|  Royal_Caribbean|   23|\n",
      "|         Carnival|   22|\n",
      "|         Princess|   17|\n",
      "| Holland_American|   14|\n",
      "|        Norwegian|   13|\n",
      "|            Costa|   11|\n",
      "|        Celebrity|   10|\n",
      "|              MSC|    8|\n",
      "|              P&O|    6|\n",
      "|             Star|    6|\n",
      "|Regent_Seven_Seas|    5|\n",
      "|        Silversea|    4|\n",
      "|           Cunard|    3|\n",
      "|         Seabourn|    3|\n",
      "|         Windstar|    3|\n",
      "|          Oceania|    3|\n",
      "|          Crystal|    2|\n",
      "|           Disney|    2|\n",
      "|          Azamara|    2|\n",
      "|           Orient|    1|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Cruise_line').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mStringIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minputCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutputCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minputCols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutputCols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mhandleInvalid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstringOrderType\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'frequencyDesc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "A label indexer that maps a string column of labels to an ML column of label indices.\n",
      "If the input column is numeric, we cast it to string and index the string values.\n",
      "The indices are in [0, numLabels). By default, this is ordered by label frequencies\n",
      "so the most frequent label gets index 0. The ordering behavior is controlled by\n",
      "setting :py:attr:`stringOrderType`. Its default value is 'frequencyDesc'.\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\",\n",
      "...     stringOrderType=\"frequencyDesc\")\n",
      ">>> stringIndexer.setHandleInvalid(\"error\")\n",
      "StringIndexer...\n",
      ">>> model = stringIndexer.fit(stringIndDf)\n",
      ">>> model.setHandleInvalid(\"error\")\n",
      "StringIndexerModel...\n",
      ">>> td = model.transform(stringIndDf)\n",
      ">>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n",
      "...     key=lambda x: x[0])\n",
      "[(0, 0.0), (1, 2.0), (2, 1.0), (3, 0.0), (4, 0.0), (5, 1.0)]\n",
      ">>> inverter = IndexToString(inputCol=\"indexed\", outputCol=\"label2\", labels=model.labels)\n",
      ">>> itd = inverter.transform(td)\n",
      ">>> sorted(set([(i[0], str(i[1])) for i in itd.select(itd.id, itd.label2).collect()]),\n",
      "...     key=lambda x: x[0])\n",
      "[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'a'), (4, 'a'), (5, 'c')]\n",
      ">>> stringIndexerPath = temp_path + \"/string-indexer\"\n",
      ">>> stringIndexer.save(stringIndexerPath)\n",
      ">>> loadedIndexer = StringIndexer.load(stringIndexerPath)\n",
      ">>> loadedIndexer.getHandleInvalid() == stringIndexer.getHandleInvalid()\n",
      "True\n",
      ">>> modelPath = temp_path + \"/string-indexer-model\"\n",
      ">>> model.save(modelPath)\n",
      ">>> loadedModel = StringIndexerModel.load(modelPath)\n",
      ">>> loadedModel.labels == model.labels\n",
      "True\n",
      ">>> indexToStringPath = temp_path + \"/index-to-string\"\n",
      ">>> inverter.save(indexToStringPath)\n",
      ">>> loadedInverter = IndexToString.load(indexToStringPath)\n",
      ">>> loadedInverter.getLabels() == inverter.getLabels()\n",
      "True\n",
      ">>> loadedModel.transform(stringIndDf).take(1) == model.transform(stringIndDf).take(1)\n",
      "True\n",
      ">>> stringIndexer.getStringOrderType()\n",
      "'frequencyDesc'\n",
      ">>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\",\n",
      "...     stringOrderType=\"alphabetDesc\")\n",
      ">>> model = stringIndexer.fit(stringIndDf)\n",
      ">>> td = model.transform(stringIndDf)\n",
      ">>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n",
      "...     key=lambda x: x[0])\n",
      "[(0, 2.0), (1, 1.0), (2, 0.0), (3, 2.0), (4, 2.0), (5, 0.0)]\n",
      ">>> fromlabelsModel = StringIndexerModel.from_labels([\"a\", \"b\", \"c\"],\n",
      "...     inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\")\n",
      ">>> result = fromlabelsModel.transform(stringIndDf)\n",
      ">>> sorted(set([(i[0], i[1]) for i in result.select(result.id, result.indexed).collect()]),\n",
      "...     key=lambda x: x[0])\n",
      "[(0, 0.0), (1, 1.0), (2, 2.0), (3, 0.0), (4, 0.0), (5, 2.0)]\n",
      ">>> testData = sc.parallelize([Row(id=0, label1=\"a\", label2=\"e\"),\n",
      "...                            Row(id=1, label1=\"b\", label2=\"f\"),\n",
      "...                            Row(id=2, label1=\"c\", label2=\"e\"),\n",
      "...                            Row(id=3, label1=\"a\", label2=\"f\"),\n",
      "...                            Row(id=4, label1=\"a\", label2=\"f\"),\n",
      "...                            Row(id=5, label1=\"c\", label2=\"f\")], 3)\n",
      ">>> multiRowDf = spark.createDataFrame(testData)\n",
      ">>> inputs = [\"label1\", \"label2\"]\n",
      ">>> outputs = [\"index1\", \"index2\"]\n",
      ">>> stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n",
      ">>> model = stringIndexer.fit(multiRowDf)\n",
      ">>> result = model.transform(multiRowDf)\n",
      ">>> sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n",
      "...     result.index2).collect()]), key=lambda x: x[0])\n",
      "[(0, 0.0, 1.0), (1, 2.0, 0.0), (2, 1.0, 1.0), (3, 0.0, 0.0), (4, 0.0, 0.0), (5, 1.0, 0.0)]\n",
      ">>> fromlabelsModel = StringIndexerModel.from_arrays_of_labels([[\"a\", \"b\", \"c\"], [\"e\", \"f\"]],\n",
      "...     inputCols=inputs, outputCols=outputs)\n",
      ">>> result = fromlabelsModel.transform(multiRowDf)\n",
      ">>> sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n",
      "...     result.index2).collect()]), key=lambda x: x[0])\n",
      "[(0, 0.0, 0.0), (1, 1.0, 1.0), (2, 2.0, 0.0), (3, 0.0, 1.0), (4, 0.0, 1.0), (5, 2.0, 1.0)]\n",
      "\u001b[1;31mInit docstring:\u001b[0m __init__(self, \\*, inputCol=None, outputCol=None, inputCols=None, outputCols=None,                  handleInvalid=\"error\", stringOrderType=\"frequencyDesc\")\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\learn-ab\\learning-pyspark\\.env\\lib\\site-packages\\pyspark\\ml\\feature.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+---------------+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|cruise_line_num|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+---------------+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|           16.0|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|           16.0|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|            1.0|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|            1.0|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|            1.0|\n",
      "|    Ecstasy|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|            1.0|\n",
      "|    Elation|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|            1.0|\n",
      "|    Fantasy|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23| 9.2|            1.0|\n",
      "|Fascination|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|            1.0|\n",
      "|    Freedom|   Carnival|  6|110.23899999999999|      37.0|  9.51| 14.87|            29.79|11.5|            1.0|\n",
      "|      Glory|   Carnival| 10|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|            1.0|\n",
      "|    Holiday|   Carnival| 28|            46.052|     14.52|  7.27|  7.26|            31.72| 6.6|            1.0|\n",
      "|Imagination|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|            1.0|\n",
      "|Inspiration|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|            1.0|\n",
      "|     Legend|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49| 9.3|            1.0|\n",
      "|   Liberty*|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|            1.0|\n",
      "|    Miracle|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67|10.3|            1.0|\n",
      "|   Paradise|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|            1.0|\n",
      "|      Pride|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67| 9.3|            1.0|\n",
      "|  Sensation|   Carnival| 20|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|            1.0|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(\n",
    "    inputCol='Cruise_line',\n",
    "    outputCol='cruise_line_num'\n",
    ")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mVectorAssembler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minputCols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutputCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mhandleInvalid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "A feature transformer that merges multiple columns into a vector column.\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
      ">>> vecAssembler = VectorAssembler(outputCol=\"features\")\n",
      ">>> vecAssembler.setInputCols([\"a\", \"b\", \"c\"])\n",
      "VectorAssembler...\n",
      ">>> vecAssembler.transform(df).head().features\n",
      "DenseVector([1.0, 0.0, 3.0])\n",
      ">>> vecAssembler.setParams(outputCol=\"freqs\").transform(df).head().freqs\n",
      "DenseVector([1.0, 0.0, 3.0])\n",
      ">>> params = {vecAssembler.inputCols: [\"b\", \"a\"], vecAssembler.outputCol: \"vector\"}\n",
      ">>> vecAssembler.transform(df, params).head().vector\n",
      "DenseVector([0.0, 1.0])\n",
      ">>> vectorAssemblerPath = temp_path + \"/vector-assembler\"\n",
      ">>> vecAssembler.save(vectorAssemblerPath)\n",
      ">>> loadedAssembler = VectorAssembler.load(vectorAssemblerPath)\n",
      ">>> loadedAssembler.transform(df).head().freqs == vecAssembler.transform(df).head().freqs\n",
      "True\n",
      ">>> dfWithNullsAndNaNs = spark.createDataFrame(\n",
      "...    [(1.0, 2.0, None), (3.0, float(\"nan\"), 4.0), (5.0, 6.0, 7.0)], [\"a\", \"b\", \"c\"])\n",
      ">>> vecAssembler2 = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\",\n",
      "...    handleInvalid=\"keep\")\n",
      ">>> vecAssembler2.transform(dfWithNullsAndNaNs).show()\n",
      "+---+---+----+-------------+\n",
      "|  a|  b|   c|     features|\n",
      "+---+---+----+-------------+\n",
      "|1.0|2.0|NULL|[1.0,2.0,NaN]|\n",
      "|3.0|NaN| 4.0|[3.0,NaN,4.0]|\n",
      "|5.0|6.0| 7.0|[5.0,6.0,7.0]|\n",
      "+---+---+----+-------------+\n",
      "...\n",
      ">>> vecAssembler2.setParams(handleInvalid=\"skip\").transform(dfWithNullsAndNaNs).show()\n",
      "+---+---+---+-------------+\n",
      "|  a|  b|  c|     features|\n",
      "+---+---+---+-------------+\n",
      "|5.0|6.0|7.0|[5.0,6.0,7.0]|\n",
      "+---+---+---+-------------+\n",
      "...\n",
      "\u001b[1;31mInit docstring:\u001b[0m __init__(self, \\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\learn-ab\\learning-pyspark\\.env\\lib\\site-packages\\pyspark\\ml\\feature.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density',\n",
       " 'crew',\n",
       " 'cruise_line_num']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|crew|\n",
      "+--------------------+----+\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[26.0,47.262,14.8...| 6.7|\n",
      "|[11.0,110.0,29.74...|19.1|\n",
      "|[17.0,101.353,26....|10.0|\n",
      "|[22.0,70.367,20.5...| 9.2|\n",
      "|[15.0,70.367,20.5...| 9.2|\n",
      "|[23.0,70.367,20.5...| 9.2|\n",
      "|[19.0,70.367,20.5...| 9.2|\n",
      "|[6.0,110.23899999...|11.5|\n",
      "|[10.0,110.0,29.74...|11.6|\n",
      "|[28.0,46.052,14.5...| 6.6|\n",
      "|[18.0,70.367,20.5...| 9.2|\n",
      "|[17.0,70.367,20.5...| 9.2|\n",
      "|[11.0,86.0,21.24,...| 9.3|\n",
      "|[8.0,110.0,29.74,...|11.6|\n",
      "|[9.0,88.5,21.24,9...|10.3|\n",
      "|[15.0,70.367,20.5...| 9.2|\n",
      "|[12.0,88.5,21.24,...| 9.3|\n",
      "|[20.0,70.367,20.5...| 9.2|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'Age',\n",
    "        'Tonnage',\n",
    "        'passengers',\n",
    "        'length',\n",
    "        'cabins',\n",
    "        'passenger_density',\n",
    "        'cruise_line_num'\n",
    "    ],\n",
    "    outputCol='features'\n",
    ")\n",
    "df_final = assembler.transform(indexed).\\\n",
    "    select('features', 'crew')\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              crew|\n",
      "+-------+------------------+\n",
      "|  count|               113|\n",
      "|   mean| 7.756991150442488|\n",
      "| stddev|3.1996308059780656|\n",
      "|    min|              0.59|\n",
      "|    max|              13.6|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|             crew|\n",
      "+-------+-----------------+\n",
      "|  count|               45|\n",
      "|   mean|7.887555555555555|\n",
      "| stddev|4.210159873640924|\n",
      "|    min|             0.59|\n",
      "|    max|             21.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = df_final.randomSplit([0.7, 0.3])\n",
    "train_data.describe().show()\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfeaturesCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabelCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpredictionCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'prediction'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxIter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mregParam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0melasticNetParam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfitIntercept\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstandardization\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msolver\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweightCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maggregationDepth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'squaredError'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.35\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxBlockSizeInMB\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Linear regression.\n",
      "\n",
      "The learning objective is to minimize the specified loss function, with regularization.\n",
      "This supports two kinds of loss:\n",
      "\n",
      "* squaredError (a.k.a squared loss)\n",
      "* huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n",
      "\n",
      "This supports multiple types of regularization:\n",
      "\n",
      "* none (a.k.a. ordinary least squares)\n",
      "* L2 (ridge regression)\n",
      "* L1 (Lasso)\n",
      "* L2 + L1 (elastic net)\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Fitting with huber loss only supports none and L2 regularization.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.ml.linalg import Vectors\n",
      ">>> df = spark.createDataFrame([\n",
      "...     (1.0, 2.0, Vectors.dense(1.0)),\n",
      "...     (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
      ">>> lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n",
      ">>> lr.setMaxIter(5)\n",
      "LinearRegression...\n",
      ">>> lr.getMaxIter()\n",
      "5\n",
      ">>> lr.setRegParam(0.1)\n",
      "LinearRegression...\n",
      ">>> lr.getRegParam()\n",
      "0.1\n",
      ">>> lr.setRegParam(0.0)\n",
      "LinearRegression...\n",
      ">>> model = lr.fit(df)\n",
      ">>> model.setFeaturesCol(\"features\")\n",
      "LinearRegressionModel...\n",
      ">>> model.setPredictionCol(\"newPrediction\")\n",
      "LinearRegressionModel...\n",
      ">>> model.getMaxIter()\n",
      "5\n",
      ">>> model.getMaxBlockSizeInMB()\n",
      "0.0\n",
      ">>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      ">>> abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n",
      "True\n",
      ">>> abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n",
      "True\n",
      ">>> abs(model.coefficients[0] - 1.0) < 0.001\n",
      "True\n",
      ">>> abs(model.intercept - 0.0) < 0.001\n",
      "True\n",
      ">>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      ">>> abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n",
      "True\n",
      ">>> lr.setParams(featuresCol=\"vector\")\n",
      "LinearRegression...\n",
      ">>> lr_path = temp_path + \"/lr\"\n",
      ">>> lr.save(lr_path)\n",
      ">>> lr2 = LinearRegression.load(lr_path)\n",
      ">>> lr2.getMaxIter()\n",
      "5\n",
      ">>> model_path = temp_path + \"/lr_model\"\n",
      ">>> model.save(model_path)\n",
      ">>> model2 = LinearRegressionModel.load(model_path)\n",
      ">>> model.coefficients[0] == model2.coefficients[0]\n",
      "True\n",
      ">>> model.intercept == model2.intercept\n",
      "True\n",
      ">>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      "True\n",
      ">>> model.numFeatures\n",
      "1\n",
      ">>> model.write().format(\"pmml\").save(model_path + \"_2\")\n",
      "\u001b[1;31mInit docstring:\u001b[0m __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                  loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\learn-ab\\learning-pyspark\\.env\\lib\\site-packages\\pyspark\\ml\\regression.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='crew',\n",
    "    predictionCol='pred'\n",
    ")\n",
    "lr_model = lr.fit(train_data)\n",
    "test_res = lr_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0177, 0.0126, -0.1259, 0.4581, 0.7432, -0.0113, 0.0612])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3992964639782035"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8656514178158731"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res.r2adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|             crew|\n",
      "+-------+-----------------+\n",
      "|  count|              158|\n",
      "|   mean|7.794177215189873|\n",
      "| stddev|3.503486564627034|\n",
      "|    min|             0.59|\n",
      "|    max|             21.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols = [\n",
    "    'Age',\n",
    "    'Tonnage',\n",
    "    'passengers',\n",
    "    'length',\n",
    "    'cabins',\n",
    "    'passenger_density'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    corr(crew, Age)|\n",
      "+-------------------+\n",
      "|-0.5306565039638852|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|corr(crew, Tonnage)|\n",
      "+-------------------+\n",
      "|  0.927568811544939|\n",
      "+-------------------+\n",
      "\n",
      "+----------------------+\n",
      "|corr(crew, passengers)|\n",
      "+----------------------+\n",
      "|    0.9152341306065384|\n",
      "+----------------------+\n",
      "\n",
      "+------------------+\n",
      "|corr(crew, length)|\n",
      "+------------------+\n",
      "|0.8958566271016579|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|corr(crew, cabins)|\n",
      "+------------------+\n",
      "|0.9508226063578497|\n",
      "+------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|corr(crew, passenger_density)|\n",
      "+-----------------------------+\n",
      "|         -0.15550928421699717|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in featureCols:\n",
    "    df.select(corr('crew', col)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
