{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local\").\\\n",
    "    appName('logistic-regression-project').\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|              Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
      "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n",
      "|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n",
      "|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|2009-03-03 23:13:37|6187 Olson Mounta...|        Kelly-Warren|    1|\n",
      "|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|2016-12-05 03:35:43|4846 Savannah Roa...|   Reynolds-Sheppard|    1|\n",
      "|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|2006-03-09 14:50:20|25271 Roy Express...|          Singh-Cole|    1|\n",
      "|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|2011-09-29 05:47:23|3725 Caroline Str...|           Lopez PLC|    1|\n",
      "|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|2006-03-28 15:42:45|363 Sandra Lodge ...|       Reed-Martinez|    1|\n",
      "|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|2016-11-13 13:13:01|Unit 8120 Box 916...|Briggs, Lamb and ...|    1|\n",
      "|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|2015-05-28 12:14:03|Unit 1895 Box 094...|    Figueroa-Maynard|    1|\n",
      "|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|2011-02-16 08:10:47|897 Kelley Overpa...|     Abbott-Thompson|    1|\n",
      "|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|2012-11-22 05:35:03|11488 Weaver Cape...|Smith, Kim and Ma...|    1|\n",
      "|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|2015-03-28 02:13:44|1774 Peter Row Ap...|Snyder, Lee and M...|    1|\n",
      "|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|2015-07-22 08:38:40|45408 David Path ...|      Sanders-Pierce|    1|\n",
      "|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|2006-09-03 06:13:55|28216 Wright Moun...|Andrews, Adams an...|    1|\n",
      "|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|2006-10-22 04:42:38|Unit 4948 Box 481...|Morgan, Phillips ...|    1|\n",
      "|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|2015-10-07 00:27:10|69203 Crosby Divi...|      Villanueva LLC|    1|\n",
      "|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|2014-11-06 23:47:14|9569 Caldwell Cre...|Berry, Orr and Ca...|    1|\n",
      "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    path='D:/learn-ab/learning-PySpark/sample-data/customer-churn.csv',\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                 900|                 900|                900|\n",
      "|   mean|         NULL|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|                NULL|                NULL|0.16666666666666666|\n",
      "| stddev|         NULL|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|                NULL|                NULL| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|churn|\n",
      "+--------------------+-----+\n",
      "|[42.0,11066.8,0.0...|    1|\n",
      "|[41.0,11916.22,0....|    1|\n",
      "|[38.0,12884.75,0....|    1|\n",
      "|[42.0,8010.76,0.0...|    1|\n",
      "|[37.0,9191.58,0.0...|    1|\n",
      "|[48.0,10356.02,0....|    1|\n",
      "|[44.0,11331.58,1....|    1|\n",
      "|[32.0,9885.12,1.0...|    1|\n",
      "|[43.0,14062.6,1.0...|    1|\n",
      "|[40.0,8066.94,1.0...|    1|\n",
      "|[30.0,11575.37,1....|    1|\n",
      "|[45.0,8771.02,1.0...|    1|\n",
      "|[45.0,8988.67,1.0...|    1|\n",
      "|[40.0,8283.32,1.0...|    1|\n",
      "|[41.0,6569.87,1.0...|    1|\n",
      "|[38.0,10494.82,1....|    1|\n",
      "|[45.0,8213.41,1.0...|    1|\n",
      "|[43.0,11226.88,0....|    1|\n",
      "|[53.0,5515.09,0.0...|    1|\n",
      "|[46.0,8046.4,1.0,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'Age',\n",
    "        'Total_Purchase',\n",
    "        'Account_Manager',\n",
    "        'Years',\n",
    "        'Num_Sites'\n",
    "    ],\n",
    "    outputCol='features'\n",
    ")\n",
    "df_final = assembler.transform(df).\\\n",
    "    select('features', 'churn')\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              churn|\n",
      "+-------+-------------------+\n",
      "|  count|                619|\n",
      "|   mean|0.16962843295638125|\n",
      "| stddev|0.37560956830021414|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|             churn|\n",
      "+-------+------------------+\n",
      "|  count|               281|\n",
      "|   mean|0.1601423487544484|\n",
      "| stddev|0.3673923277936089|\n",
      "|    min|                 0|\n",
      "|    max|                 1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = df_final.randomSplit([0.7, 0.3])\n",
    "train_data.describe().show()\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfeaturesCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabelCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpredictionCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'prediction'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxIter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mregParam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0melasticNetParam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfitIntercept\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mthresholds\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mprobabilityCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'probability'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrawPredictionCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'rawPrediction'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstandardization\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweightCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maggregationDepth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfamily\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlowerBoundsOnCoefficients\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMatrix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mupperBoundsOnCoefficients\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMatrix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlowerBoundsOnIntercepts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVector\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mupperBoundsOnIntercepts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVector\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxBlockSizeInMB\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Logistic regression.\n",
      "This class supports multinomial logistic (softmax) and binomial logistic regression.\n",
      "\n",
      ".. versionadded:: 1.3.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.sql import Row\n",
      ">>> from pyspark.ml.linalg import Vectors\n",
      ">>> bdf = sc.parallelize([\n",
      "...     Row(label=1.0, weight=1.0, features=Vectors.dense(0.0, 5.0)),\n",
      "...     Row(label=0.0, weight=2.0, features=Vectors.dense(1.0, 2.0)),\n",
      "...     Row(label=1.0, weight=3.0, features=Vectors.dense(2.0, 1.0)),\n",
      "...     Row(label=0.0, weight=4.0, features=Vectors.dense(3.0, 3.0))]).toDF()\n",
      ">>> blor = LogisticRegression(weightCol=\"weight\")\n",
      ">>> blor.getRegParam()\n",
      "0.0\n",
      ">>> blor.setRegParam(0.01)\n",
      "LogisticRegression...\n",
      ">>> blor.getRegParam()\n",
      "0.01\n",
      ">>> blor.setMaxIter(10)\n",
      "LogisticRegression...\n",
      ">>> blor.getMaxIter()\n",
      "10\n",
      ">>> blor.clear(blor.maxIter)\n",
      ">>> blorModel = blor.fit(bdf)\n",
      ">>> blorModel.setFeaturesCol(\"features\")\n",
      "LogisticRegressionModel...\n",
      ">>> blorModel.setProbabilityCol(\"newProbability\")\n",
      "LogisticRegressionModel...\n",
      ">>> blorModel.getProbabilityCol()\n",
      "'newProbability'\n",
      ">>> blorModel.getMaxBlockSizeInMB()\n",
      "0.0\n",
      ">>> blorModel.setThreshold(0.1)\n",
      "LogisticRegressionModel...\n",
      ">>> blorModel.getThreshold()\n",
      "0.1\n",
      ">>> blorModel.coefficients\n",
      "DenseVector([-1.080..., -0.646...])\n",
      ">>> blorModel.intercept\n",
      "3.112...\n",
      ">>> blorModel.evaluate(bdf).accuracy == blorModel.summary.accuracy\n",
      "True\n",
      ">>> data_path = \"data/mllib/sample_multiclass_classification_data.txt\"\n",
      ">>> mdf = spark.read.format(\"libsvm\").load(data_path)\n",
      ">>> mlor = LogisticRegression(regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n",
      ">>> mlorModel = mlor.fit(mdf)\n",
      ">>> mlorModel.coefficientMatrix\n",
      "SparseMatrix(3, 4, [0, 1, 2, 3], [3, 2, 1], [1.87..., -2.75..., -0.50...], 1)\n",
      ">>> mlorModel.interceptVector\n",
      "DenseVector([0.04..., -0.42..., 0.37...])\n",
      ">>> test0 = sc.parallelize([Row(features=Vectors.dense(-1.0, 1.0))]).toDF()\n",
      ">>> blorModel.predict(test0.head().features)\n",
      "1.0\n",
      ">>> blorModel.predictRaw(test0.head().features)\n",
      "DenseVector([-3.54..., 3.54...])\n",
      ">>> blorModel.predictProbability(test0.head().features)\n",
      "DenseVector([0.028, 0.972])\n",
      ">>> result = blorModel.transform(test0).head()\n",
      ">>> result.prediction\n",
      "1.0\n",
      ">>> result.newProbability\n",
      "DenseVector([0.02..., 0.97...])\n",
      ">>> result.rawPrediction\n",
      "DenseVector([-3.54..., 3.54...])\n",
      ">>> test1 = sc.parallelize([Row(features=Vectors.sparse(2, [0], [1.0]))]).toDF()\n",
      ">>> blorModel.transform(test1).head().prediction\n",
      "1.0\n",
      ">>> blor.setParams(\"vector\")\n",
      "Traceback (most recent call last):\n",
      "    ...\n",
      "TypeError: Method setParams forces keyword arguments.\n",
      ">>> lr_path = temp_path + \"/lr\"\n",
      ">>> blor.save(lr_path)\n",
      ">>> lr2 = LogisticRegression.load(lr_path)\n",
      ">>> lr2.getRegParam()\n",
      "0.01\n",
      ">>> model_path = temp_path + \"/lr_model\"\n",
      ">>> blorModel.save(model_path)\n",
      ">>> model2 = LogisticRegressionModel.load(model_path)\n",
      ">>> blorModel.coefficients[0] == model2.coefficients[0]\n",
      "True\n",
      ">>> blorModel.intercept == model2.intercept\n",
      "True\n",
      ">>> model2\n",
      "LogisticRegressionModel: uid=..., numClasses=2, numFeatures=2\n",
      ">>> blorModel.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      "True\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "__init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  threshold=0.5, thresholds=None, probabilityCol=\"probability\",                  rawPredictionCol=\"rawPrediction\", standardization=True, weightCol=None,                  aggregationDepth=2, family=\"auto\",                  lowerBoundsOnCoefficients=None, upperBoundsOnCoefficients=None,                  lowerBoundsOnIntercepts=None, upperBoundsOnIntercepts=None,                  maxBlockSizeInMB=0.0):\n",
      "If the threshold and thresholds Params are both set, they must be equivalent.\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\learn-ab\\learning-pyspark\\.env\\lib\\site-packages\\pyspark\\ml\\classification.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='churn'\n",
    ")\n",
    "log_reg_model = log_reg.fit(train_data)\n",
    "log_reg_model_summary = log_reg_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+\n",
      "|summary|              churn|        prediction|\n",
      "+-------+-------------------+------------------+\n",
      "|  count|                619|               619|\n",
      "|   mean|0.16962843295638125|0.1260096930533118|\n",
      "| stddev|0.37560956830021414|0.3321286741860335|\n",
      "|    min|                0.0|               0.0|\n",
      "|    max|                1.0|               1.0|\n",
      "+-------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_model_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8939.61,0.0...|    0|[6.18032267755672...|[0.99793451517581...|       0.0|\n",
      "|[29.0,9617.59,0.0...|    0|[4.30699904617236...|[0.98670520938679...|       0.0|\n",
      "|[29.0,13240.01,1....|    0|[6.45844656169183...|[0.99843522394845...|       0.0|\n",
      "|[30.0,8874.83,0.0...|    0|[3.06430927607732...|[0.95539629416520...|       0.0|\n",
      "|[30.0,10960.52,1....|    0|[2.22029705761465...|[0.90205744382154...|       0.0|\n",
      "|[30.0,13473.35,0....|    0|[2.59814226631730...|[0.93074192333488...|       0.0|\n",
      "|[31.0,5304.6,0.0,...|    0|[3.30280901985182...|[0.96452505100729...|       0.0|\n",
      "|[31.0,7073.61,0.0...|    0|[2.99645559962407...|[0.95241374555238...|       0.0|\n",
      "|[31.0,8829.83,1.0...|    0|[4.20075638701755...|[0.98523697405454...|       0.0|\n",
      "|[31.0,10058.87,1....|    0|[4.26531060321243...|[0.98614709546581...|       0.0|\n",
      "|[31.0,10182.6,1.0...|    0|[4.54077762758273...|[0.98944743442650...|       0.0|\n",
      "|[31.0,11297.57,1....|    1|[0.82685710111410...|[0.69568997056648...|       0.0|\n",
      "|[32.0,6367.22,1.0...|    0|[2.77126376634853...|[0.94110307373624...|       0.0|\n",
      "|[32.0,8575.71,0.0...|    0|[3.65771589953821...|[0.97485711357677...|       0.0|\n",
      "|[32.0,10716.75,0....|    0|[4.29739320419183...|[0.98657860866365...|       0.0|\n",
      "|[32.0,12254.75,1....|    0|[2.39838756532668...|[0.91670426463298...|       0.0|\n",
      "|[32.0,12403.6,0.0...|    0|[5.47931098824266...|[0.99584513155130...|       0.0|\n",
      "|[33.0,4711.89,0.0...|    0|[5.75755030701003...|[0.99685110653456...|       0.0|\n",
      "|[33.0,5738.82,0.0...|    0|[4.29283031448050...|[0.98651805593123...|       0.0|\n",
      "|[33.0,10306.21,1....|    0|[1.87680941753308...|[0.86724421955261...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_and_labels = log_reg_model.evaluate(test_data)\n",
    "pred_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrawPredictionCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'rawPrediction'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabelCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetricName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'BinaryClassificationEvaluatorMetricType'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'areaUnderROC'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweightCol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnumBins\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Evaluator for binary classification, which expects input columns rawPrediction, label\n",
      "and an optional weight column.\n",
      "The rawPrediction column can be of type double (binary 0/1 prediction, or probability of label\n",
      "1) or of type vector (length-2 vector of raw predictions, scores, or label probabilities).\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.ml.linalg import Vectors\n",
      ">>> scoreAndLabels = map(lambda x: (Vectors.dense([1.0 - x[0], x[0]]), x[1]),\n",
      "...    [(0.1, 0.0), (0.1, 1.0), (0.4, 0.0), (0.6, 0.0), (0.6, 1.0), (0.6, 1.0), (0.8, 1.0)])\n",
      ">>> dataset = spark.createDataFrame(scoreAndLabels, [\"raw\", \"label\"])\n",
      "...\n",
      ">>> evaluator = BinaryClassificationEvaluator()\n",
      ">>> evaluator.setRawPredictionCol(\"raw\")\n",
      "BinaryClassificationEvaluator...\n",
      ">>> evaluator.evaluate(dataset)\n",
      "0.70...\n",
      ">>> evaluator.evaluate(dataset, {evaluator.metricName: \"areaUnderPR\"})\n",
      "0.83...\n",
      ">>> bce_path = temp_path + \"/bce\"\n",
      ">>> evaluator.save(bce_path)\n",
      ">>> evaluator2 = BinaryClassificationEvaluator.load(bce_path)\n",
      ">>> str(evaluator2.getRawPredictionCol())\n",
      "'raw'\n",
      ">>> scoreAndLabelsAndWeight = map(lambda x: (Vectors.dense([1.0 - x[0], x[0]]), x[1], x[2]),\n",
      "...    [(0.1, 0.0, 1.0), (0.1, 1.0, 0.9), (0.4, 0.0, 0.7), (0.6, 0.0, 0.9),\n",
      "...     (0.6, 1.0, 1.0), (0.6, 1.0, 0.3), (0.8, 1.0, 1.0)])\n",
      ">>> dataset = spark.createDataFrame(scoreAndLabelsAndWeight, [\"raw\", \"label\", \"weight\"])\n",
      "...\n",
      ">>> evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"raw\", weightCol=\"weight\")\n",
      ">>> evaluator.evaluate(dataset)\n",
      "0.70...\n",
      ">>> evaluator.evaluate(dataset, {evaluator.metricName: \"areaUnderPR\"})\n",
      "0.82...\n",
      ">>> evaluator.getNumBins()\n",
      "1000\n",
      "\u001b[1;31mInit docstring:\u001b[0m __init__(self, \\*, rawPredictionCol=\"rawPrediction\", labelCol=\"label\",                  metricName=\"areaUnderROC\", weightCol=None, numBins=1000)\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\learn-ab\\learning-pyspark\\.env\\lib\\site-packages\\pyspark\\ml\\evaluation.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_eval = BinaryClassificationEvaluator(\n",
    "    labelCol='churn',\n",
    "    rawPredictionCol='prediction'\n",
    ")\n",
    "auc = log_reg_model_eval.evaluate(pred_and_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8052730696798494"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|\n",
      "|Megan Ferguson|32.0|        6487.5|              0|  9.4|     14.0|2016-10-28 05:32:13|922 Wright Branch...|   Sexton-Golden|\n",
      "|  Taylor Young|32.0|      13147.71|              1| 10.0|      8.0|2012-03-20 00:36:46|Unit 0789 Box 073...|        Wood LLC|\n",
      "| Jessica Drake|22.0|       8445.26|              1| 3.46|     14.0|2011-02-04 19:29:27|1148 Tina Straven...|   Parks-Robbins|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cust = spark.read.csv(\n",
    "    path='D:/learn-ab/learning-PySpark/sample-data/new-customers.csv',\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "new_cust.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cust.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+--------------------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|            features|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+--------------------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|[37.0,9935.53,1.0...|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson|[23.0,7526.94,1.0...|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|[65.0,100.0,1.0,1...|\n",
      "|Megan Ferguson|32.0|        6487.5|              0|  9.4|     14.0|2016-10-28 05:32:13|922 Wright Branch...|   Sexton-Golden|[32.0,6487.5,0.0,...|\n",
      "|  Taylor Young|32.0|      13147.71|              1| 10.0|      8.0|2012-03-20 00:36:46|Unit 0789 Box 073...|        Wood LLC|[32.0,13147.71,1....|\n",
      "| Jessica Drake|22.0|       8445.26|              1| 3.46|     14.0|2011-02-04 19:29:27|1148 Tina Straven...|   Parks-Robbins|[22.0,8445.26,1.0...|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cust_final = assembler.transform(new_cust)\n",
    "new_cust_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cust_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+--------------------+----------------+\n",
      "|summary|        Names|               Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|         Company|\n",
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+--------------------+----------------+\n",
      "|  count|            6|                 6|                6|                 6|                6|                 6|                   6|               6|\n",
      "|   mean|         NULL|35.166666666666664|7607.156666666667|0.8333333333333334|6.808333333333334|12.333333333333334|                NULL|            NULL|\n",
      "| stddev|         NULL| 15.71517313511584|4346.008232825459| 0.408248290463863|3.708737880555414|3.3862466931200785|                NULL|            NULL|\n",
      "|    min|Andrew Mccall|              22.0|            100.0|                 0|              1.0|               8.0|085 Austin Views ...|Barron-Robertson|\n",
      "|    max| Taylor Young|              65.0|         13147.71|                 1|             10.0|              15.0|Unit 0789 Box 073...|        Wood LLC|\n",
      "+-------+-------------+------------------+-----------------+------------------+-----------------+------------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cust_final.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------+\n",
      "|         Names|         Company|prediction|\n",
      "+--------------+----------------+----------+\n",
      "| Andrew Mccall|        King Ltd|       0.0|\n",
      "|Michele Wright|   Cannon-Benson|       1.0|\n",
      "|  Jeremy Chang|Barron-Robertson|       1.0|\n",
      "|Megan Ferguson|   Sexton-Golden|       1.0|\n",
      "|  Taylor Young|        Wood LLC|       0.0|\n",
      "| Jessica Drake|   Parks-Robbins|       1.0|\n",
      "+--------------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_log_reg_model = log_reg.fit(df_final)\n",
    "new_cust_res = final_log_reg_model.transform(new_cust_final)\n",
    "new_cust_res.select('Names', 'Company', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
