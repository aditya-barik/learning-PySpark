{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master('local').\\\n",
    "    appName('nlp-project').\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    path='D:/learn-ab/learning-PySpark/sample-data/sms-spam-collection.tsv',\n",
    "    inferSchema=True,\n",
    "    sep='\\t'\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('length',length(df['text']))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    CountVectorizer,\n",
    "    IDF,\n",
    "    StringIndexer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    inputCol='text',\n",
    "    outputCol='token_text'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_remover = StopWordsRemover(\n",
    "    inputCol='token_text',\n",
    "    outputCol='stop_tokens'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(\n",
    "    inputCol='stop_tokens',\n",
    "    outputCol='count_vec'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(\n",
    "    inputCol='count_vec',\n",
    "    outputCol='tf_idf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_spam_to_num = StringIndexer(\n",
    "    inputCol='class',\n",
    "    outputCol='label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'tf_idf',\n",
    "        'length'\n",
    "    ],\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(\n",
    "    stages=[\n",
    "        ham_spam_to_num,\n",
    "        tokenizer,\n",
    "        stop_words_remover,\n",
    "        count_vec,\n",
    "        idf,\n",
    "        assembler\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipeline = data_prep_pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                text|length|label|          token_text|         stop_tokens|           count_vec|              tf_idf|            features|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|(13424,[7,11,31,6...|\n",
      "|  ham|Ok lar... Joking ...|    29|  0.0|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,301,...|(13423,[0,24,301,...|(13424,[0,24,301,...|\n",
      "| spam|Free entry in 2 a...|   155|  1.0|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|(13424,[2,13,19,3...|\n",
      "|  ham|U dun say so earl...|    49|  0.0|[u, dun, say, so,...|[u, dun, say, ear...|(13423,[0,70,80,1...|(13423,[0,70,80,1...|(13424,[0,70,80,1...|\n",
      "|  ham|Nah I don't think...|    61|  0.0|[nah, i, don't, t...|[nah, think, goes...|(13423,[36,134,31...|(13423,[36,134,31...|(13424,[36,134,31...|\n",
      "| spam|FreeMsg Hey there...|   147|  1.0|[freemsg, hey, th...|[freemsg, hey, da...|(13423,[10,60,140...|(13423,[10,60,140...|(13424,[10,60,140...|\n",
      "|  ham|Even my brother i...|    77|  0.0|[even, my, brothe...|[even, brother, l...|(13423,[10,53,102...|(13423,[10,53,102...|(13424,[10,53,102...|\n",
      "|  ham|As per your reque...|   160|  0.0|[as, per, your, r...|[per, request, 'm...|(13423,[127,185,4...|(13423,[127,185,4...|(13424,[127,185,4...|\n",
      "| spam|WINNER!! As a val...|   157|  1.0|[winner!!, as, a,...|[winner!!, valued...|(13423,[1,47,121,...|(13423,[1,47,121,...|(13424,[1,47,121,...|\n",
      "| spam|Had your mobile 1...|   154|  1.0|[had, your, mobil...|[mobile, 11, mont...|(13423,[0,1,13,27...|(13423,[0,1,13,27...|(13424,[0,1,13,27...|\n",
      "|  ham|I'm gonna be home...|   109|  0.0|[i'm, gonna, be, ...|[gonna, home, soo...|(13423,[18,43,117...|(13423,[18,43,117...|(13424,[18,43,117...|\n",
      "| spam|SIX chances to wi...|   136|  1.0|[six, chances, to...|[six, chances, wi...|(13423,[8,16,37,8...|(13423,[8,16,37,8...|(13424,[8,16,37,8...|\n",
      "| spam|URGENT! You have ...|   155|  1.0|[urgent!, you, ha...|[urgent!, won, 1,...|(13423,[13,30,47,...|(13423,[13,30,47,...|(13424,[13,30,47,...|\n",
      "|  ham|I've been searchi...|   196|  0.0|[i've, been, sear...|[searching, right...|(13423,[39,95,221...|(13423,[39,95,221...|(13424,[39,95,221...|\n",
      "|  ham|I HAVE A DATE ON ...|    35|  0.0|[i, have, a, date...|[date, sunday, wi...|(13423,[555,1797,...|(13423,[555,1797,...|(13424,[555,1797,...|\n",
      "| spam|XXXMobileMovieClu...|   149|  1.0|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(13423,[30,109,11...|(13423,[30,109,11...|(13424,[30,109,11...|\n",
      "|  ham|Oh k...i'm watchi...|    26|  0.0|[oh, k...i'm, wat...|[oh, k...i'm, wat...|(13423,[82,214,44...|(13423,[82,214,44...|(13424,[82,214,44...|\n",
      "|  ham|Eh u remember how...|    81|  0.0|[eh, u, remember,...|[eh, u, remember,...|(13423,[0,2,49,13...|(13423,[0,2,49,13...|(13424,[0,2,49,13...|\n",
      "|  ham|Fine if thats th...|    56|  0.0|[fine, if, thats...|[fine, thats, wa...|(13423,[0,74,105,...|(13423,[0,74,105,...|(13424,[0,74,105,...|\n",
      "| spam|England v Macedon...|   155|  1.0|[england, v, mace...|[england, v, mace...|(13423,[4,30,33,5...|(13423,[4,30,33,5...|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = data_prep_pipeline.transform(df)\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,301,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "|  0.0|(13424,[0,70,80,1...|\n",
      "|  0.0|(13424,[36,134,31...|\n",
      "|  1.0|(13424,[10,60,140...|\n",
      "|  0.0|(13424,[10,53,102...|\n",
      "|  0.0|(13424,[127,185,4...|\n",
      "|  1.0|(13424,[1,47,121,...|\n",
      "|  1.0|(13424,[0,1,13,27...|\n",
      "|  0.0|(13424,[18,43,117...|\n",
      "|  1.0|(13424,[8,16,37,8...|\n",
      "|  1.0|(13424,[13,30,47,...|\n",
      "|  0.0|(13424,[39,95,221...|\n",
      "|  0.0|(13424,[555,1797,...|\n",
      "|  1.0|(13424,[30,109,11...|\n",
      "|  0.0|(13424,[82,214,44...|\n",
      "|  0.0|(13424,[0,2,49,13...|\n",
      "|  0.0|(13424,[0,74,105,...|\n",
      "|  1.0|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.select('label', 'features')\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_clean.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detector = naive_bayes.fit(train_data)\n",
    "spam_results = spam_detector.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,4,50,...|[-823.94824804744...|[1.0,4.5787781995...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[-885.37071029385...|[1.0,5.7886299880...|       0.0|\n",
      "|  0.0|(13424,[0,1,11,32...|[-870.71661736307...|[1.0,1.5179443996...|       0.0|\n",
      "|  0.0|(13424,[0,1,150,1...|[-250.07070457697...|[0.99996410738653...|       0.0|\n",
      "|  0.0|(13424,[0,1,881,1...|[-94.252511420495...|[0.99999999714904...|       0.0|\n",
      "|  0.0|(13424,[0,1,4305,...|[-127.83132072188...|[0.99998782514690...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3282.6953824706...|[1.0,8.2338818920...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,1...|[-1316.6433121299...|[1.0,1.9876044985...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,128...|[-645.50214154147...|[1.0,4.8157569940...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,8,3...|[-1152.1680333489...|[1.0,4.3223743916...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,11,...|[-1338.6485118521...|[1.0,5.8810252985...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,27,...|[-472.13501065692...|[0.97214633743897...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,31,...|[-657.10855907048...|[1.0,1.9297894651...|       0.0|\n",
      "|  0.0|(13424,[0,2,8,12,...|[-1321.0894840065...|[1.0,6.8983735180...|       0.0|\n",
      "|  0.0|(13424,[0,2,8,14,...|[-480.23856486201...|[1.0,2.1044986124...|       0.0|\n",
      "|  0.0|(13424,[0,2,8,20,...|[-805.81489332876...|[1.0,1.4595588075...|       0.0|\n",
      "|  0.0|(13424,[0,2,8,28,...|[-1327.4863098721...|[1.0,6.2807138951...|       0.0|\n",
      "|  0.0|(13424,[0,2,8,199...|[-572.34947736580...|[1.0,1.3873924640...|       0.0|\n",
      "|  0.0|(13424,[0,2,9,14,...|[-609.74602394908...|[1.0,7.7799387835...|       0.0|\n",
      "|  0.0|(13424,[0,2,10,70...|[-835.45916066940...|[1.0,1.1161727800...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_eval = MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction',\n",
    "    labelCol='label',\n",
    "    metricName='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detection_f1_score = spam_eval.evaluate(spam_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of model at predicting spam was: 0.93\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score of model at predicting spam was: {spam_detection_f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
